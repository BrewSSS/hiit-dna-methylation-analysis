{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to acquire DNA methylation data from the Gene Expression Omnibus (GEO) database for High-Intensity Interval Training (HIIT) epigenetic response analysis. We will download and process the GSE171140 dataset, which contains Illumina EPIC 850K methylation array data from skeletal muscle samples collected at multiple time points during a HIIT intervention study.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Download GEO series matrix files programmatically\n",
    "2. Extract and parse methylation data from compressed archives\n",
    "3. Parse sample metadata to understand experimental design\n",
    "4. Create structured sample mappings for downstream analysis\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The GSE171140 dataset examines epigenetic adaptations to HIIT in human skeletal muscle, including:\n",
    "\n",
    "- **Timepoints**: Baseline (PRE), 4 weeks (4WP), 8 weeks (8WP), and 12 weeks (12WP) of training\n",
    "- **Control period**: Measurements after a non-training control period\n",
    "- **Platform**: Illumina Infinium MethylationEPIC BeadChip (850K CpG sites)\n",
    "- **Tissue**: Skeletal muscle biopsies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we import the necessary modules from our package. The `src.data` module provides utilities for downloading, loading, and parsing GEO data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project-specific imports\n",
    "from src.data.loader import GEODataLoader\n",
    "from src.data.sample_mapping import SampleMapper\n",
    "\n",
    "# Configure logging for informative output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the Data Loader\n",
    "\n",
    "The `GEODataLoader` class handles all aspects of acquiring GEO data:\n",
    "\n",
    "- Constructing the correct FTP download URL\n",
    "- Downloading compressed series matrix files with progress tracking\n",
    "- Extracting gzip-compressed files\n",
    "- Loading the methylation data matrix into pandas DataFrames\n",
    "\n",
    "We specify the GEO accession ID and the directory where data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directories\n",
    "data_dir = project_root / 'data' / 'raw'\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize the GEO data loader\n",
    "# The metadata_lines parameter specifies how many lines of metadata\n",
    "# precede the actual data matrix (74 is typical for GEO series matrix files)\n",
    "loader = GEODataLoader(\n",
    "    geo_accession='GSE171140',\n",
    "    data_dir=data_dir,\n",
    "    metadata_lines=74\n",
    ")\n",
    "\n",
    "print(f\"Data directory: {loader.data_dir}\")\n",
    "print(f\"Expected file: {loader.series_matrix_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Series Matrix File\n",
    "\n",
    "The series matrix file contains both metadata (sample information, experimental design) and the actual methylation beta values. This file is typically large (hundreds of MB to several GB for EPIC arrays) and is provided in gzip-compressed format.\n",
    "\n",
    "The download function:\n",
    "- Checks if the file already exists (skips download if present)\n",
    "- Shows download progress with estimated time remaining\n",
    "- Handles network errors gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the series matrix file from GEO\n",
    "# This may take several minutes depending on your internet connection\n",
    "# The file is approximately 1 GB compressed\n",
    "\n",
    "gz_path = loader.download_series_matrix(force=False)\n",
    "\n",
    "print(f\"\\nCompressed file location: {gz_path}\")\n",
    "print(f\"File size: {gz_path.stat().st_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Compressed File\n",
    "\n",
    "After downloading, we extract the gzip-compressed file. The uncompressed series matrix file will be significantly larger than the compressed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the compressed file\n",
    "# This creates the uncompressed .txt file\n",
    "\n",
    "extracted_path = loader.extract_gz_file(force=False)\n",
    "\n",
    "print(f\"Extracted file: {extracted_path}\")\n",
    "print(f\"Uncompressed size: {extracted_path.stat().st_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preview File Contents\n",
    "\n",
    "Before loading the full dataset, it's useful to preview the file structure. GEO series matrix files have a specific format:\n",
    "\n",
    "1. **Metadata section** (lines starting with `!`): Contains sample information, experimental design, platform details\n",
    "2. **Data header**: Column names (sample IDs)\n",
    "3. **Data matrix**: Rows are probe IDs (CpG sites), columns are samples, values are beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first lines of the file to understand its structure\n",
    "preview_lines = loader.preview_file(num_lines=30)\n",
    "\n",
    "print(\"File structure preview:\")\n",
    "print(\"=\" * 80)\n",
    "for i, line in enumerate(preview_lines[:20]):\n",
    "    # Truncate long lines for display\n",
    "    display_line = line[:100] + \"...\" if len(line) > 100 else line\n",
    "    print(f\"{i+1:3d}: {display_line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Sample Metadata\n",
    "\n",
    "The metadata section contains crucial information about each sample:\n",
    "\n",
    "- **Sample_geo_accession**: Unique GEO identifier (GSMxxxxxx)\n",
    "- **Sample_title**: Descriptive sample name with experimental details\n",
    "- **Sample_source_name_ch1**: Tissue/source description\n",
    "- **Sample_characteristics_ch1**: Key-value pairs with age, sex, treatment, etc.\n",
    "\n",
    "This information is essential for creating labels for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata from the series matrix file\n",
    "metadata = loader.get_metadata()\n",
    "\n",
    "# Display available metadata fields\n",
    "print(\"Available metadata fields:\")\n",
    "print(\"-\" * 40)\n",
    "for field_name in sorted(metadata.keys()):\n",
    "    num_values = len(metadata[field_name])\n",
    "    print(f\"  {field_name}: {num_values} values\")\n",
    "\n",
    "# Show sample count\n",
    "n_samples = len(metadata.get('Sample_geo_accession', []))\n",
    "print(f\"\\nTotal samples: {n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview sample titles to understand naming convention\n",
    "print(\"Sample title examples:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "sample_titles = metadata.get('Sample_title', [])\n",
    "for i, title in enumerate(sample_titles[:10]):\n",
    "    sample_id = metadata['Sample_geo_accession'][i]\n",
    "    print(f\"  {sample_id}: {title}\")\n",
    "\n",
    "print(\"\\n... (showing first 10 of many samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Sample Mapping\n",
    "\n",
    "The `SampleMapper` class parses sample names and metadata to create a structured mapping table. This is critical for:\n",
    "\n",
    "1. **Binary classification**: HIIT intervention vs. Control/Baseline\n",
    "2. **Multiclass classification**: Different HIIT durations (4W, 8W, 12W)\n",
    "3. **Time-series analysis**: Tracking changes within individuals over time\n",
    "\n",
    "The mapper automatically:\n",
    "- Extracts timepoint information from sample names\n",
    "- Identifies individual subjects for paired analyses\n",
    "- Parses demographic information (age, sex) from characteristics\n",
    "- Creates classification labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sample mapper\n",
    "mapper = SampleMapper()\n",
    "\n",
    "# Create comprehensive sample mapping from metadata\n",
    "sample_mapping = mapper.create_sample_mapping(\n",
    "    metadata,\n",
    "    output_path=str(data_dir / 'GSE171140_sample_mapping.csv')\n",
    ")\n",
    "\n",
    "# Display the mapping structure\n",
    "print(\"Sample mapping columns:\")\n",
    "print(sample_mapping.columns.tolist())\n",
    "print(f\"\\nTotal samples mapped: {len(sample_mapping)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the sample mapping table\n",
    "# Note: Specific values will depend on your dataset\n",
    "\n",
    "display_cols = [\n",
    "    'sample_id', 'sample_name', 'individual_id', \n",
    "    'time_point', 'binary_class', 'multi_class'\n",
    "]\n",
    "\n",
    "print(\"Sample mapping preview:\")\n",
    "print(\"=\" * 100)\n",
    "print(sample_mapping[display_cols].head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Experimental Design\n",
    "\n",
    "Understanding the experimental design is crucial for proper statistical analysis. We examine:\n",
    "\n",
    "- Distribution of samples across timepoints\n",
    "- Balance between experimental groups\n",
    "- Number of unique individuals (for paired analyses)\n",
    "- Demographic characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timepoint distribution\n",
    "print(\"Timepoint Distribution:\")\n",
    "print(\"-\" * 40)\n",
    "timepoint_counts = sample_mapping['time_point'].value_counts()\n",
    "for tp, count in timepoint_counts.items():\n",
    "    pct = count / len(sample_mapping) * 100\n",
    "    print(f\"  {tp}: {count} samples ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification balance\n",
    "print(\"\\nBinary Classification Distribution:\")\n",
    "print(\"-\" * 40)\n",
    "binary_counts = sample_mapping['binary_class'].value_counts()\n",
    "for cls, count in binary_counts.items():\n",
    "    pct = count / len(sample_mapping) * 100\n",
    "    print(f\"  {cls}: {count} samples ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass distribution (HIIT duration categories)\n",
    "print(\"\\nMulticlass Distribution (HIIT samples only):\")\n",
    "print(\"-\" * 40)\n",
    "multi_counts = sample_mapping['multi_class'].value_counts(dropna=False)\n",
    "for cls, count in multi_counts.items():\n",
    "    if pd.isna(cls):\n",
    "        print(f\"  Non-HIIT (Baseline/Control): {count} samples\")\n",
    "    else:\n",
    "        print(f\"  {cls} HIIT: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual subject count\n",
    "unique_individuals = sample_mapping['individual_id'].nunique()\n",
    "print(f\"\\nUnique individuals: {unique_individuals}\")\n",
    "\n",
    "# Check for samples per individual (for repeated measures design)\n",
    "samples_per_individual = sample_mapping.groupby('individual_id').size()\n",
    "print(f\"Samples per individual: min={samples_per_individual.min()}, \"\n",
    "      f\"max={samples_per_individual.max()}, median={samples_per_individual.median():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Classification Labels\n",
    "\n",
    "We create numeric labels suitable for machine learning algorithms:\n",
    "\n",
    "- **Binary labels**: 0 = Control/Baseline, 1 = HIIT intervention\n",
    "- **Multiclass labels**: 0 = 4W HIIT, 1 = 8W HIIT, 2 = 12W HIIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary classification labels\n",
    "binary_labels = mapper.create_binary_labels(\n",
    "    sample_mapping,\n",
    "    positive_class='HIIT',\n",
    "    exclude_unknown=True\n",
    ")\n",
    "\n",
    "print(\"Binary labels distribution:\")\n",
    "print(binary_labels.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiclass labels for HIIT duration\n",
    "multiclass_labels = mapper.create_multiclass_labels(\n",
    "    sample_mapping,\n",
    "    include_baseline=False\n",
    ")\n",
    "\n",
    "print(\"Multiclass labels distribution (HIIT samples only):\")\n",
    "print(multiclass_labels.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load Methylation Data Matrix\n",
    "\n",
    "Finally, we load the actual methylation beta values. This is the most memory-intensive step, as the EPIC array contains approximately 850,000 CpG sites measured across all samples.\n",
    "\n",
    "**Note**: For very large datasets, consider loading a subset of probes initially for testing, then loading the full dataset for final analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the methylation data matrix\n",
    "# This may take several minutes for EPIC array data\n",
    "\n",
    "print(\"Loading methylation data matrix...\")\n",
    "print(\"This may take several minutes for large datasets.\")\n",
    "\n",
    "methylation_data = loader.load_methylation_matrix()\n",
    "\n",
    "print(f\"\\nData matrix dimensions:\")\n",
    "print(f\"  CpG probes (rows): {methylation_data.shape[0]:,}\")\n",
    "print(f\"  Samples (columns): {methylation_data.shape[1]}\")\n",
    "print(f\"  Total values: {methylation_data.size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the methylation data structure\n",
    "print(\"Methylation data preview:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nProbe ID index examples: {list(methylation_data.index[:5])}\")\n",
    "print(f\"Sample ID column examples: {list(methylation_data.columns[:5])}\")\n",
    "\n",
    "# Check beta value range\n",
    "print(f\"\\nBeta value statistics:\")\n",
    "print(f\"  Min: {methylation_data.min().min():.4f}\")\n",
    "print(f\"  Max: {methylation_data.max().max():.4f}\")\n",
    "print(f\"  Mean: {methylation_data.mean().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Processed Data\n",
    "\n",
    "Save the sample mapping and basic data information for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save sample mapping (already saved during creation, but we can confirm)\n",
    "mapping_path = data_dir / 'GSE171140_sample_mapping.csv'\n",
    "print(f\"Sample mapping saved to: {mapping_path}\")\n",
    "\n",
    "# Save acquisition info for reproducibility\n",
    "acquisition_info = {\n",
    "    'geo_accession': 'GSE171140',\n",
    "    'n_samples': len(sample_mapping),\n",
    "    'n_probes': methylation_data.shape[0],\n",
    "    'timepoints': sample_mapping['time_point'].unique().tolist(),\n",
    "    'data_file': str(loader.series_matrix_path)\n",
    "}\n",
    "\n",
    "info_path = processed_dir / 'acquisition_info.json'\n",
    "with open(info_path, 'w') as f:\n",
    "    json.dump(acquisition_info, f, indent=2)\n",
    "\n",
    "print(f\"Acquisition info saved to: {info_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we successfully:\n",
    "\n",
    "1. Downloaded the GSE171140 series matrix file from GEO\n",
    "2. Extracted and previewed the file structure\n",
    "3. Parsed sample metadata to understand the experimental design\n",
    "4. Created a structured sample mapping with classification labels\n",
    "5. Loaded the full methylation data matrix\n",
    "6. Saved processed outputs for subsequent analysis\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to **02_preprocessing.ipynb** to:\n",
    "- Filter low-variance CpG sites\n",
    "- Handle missing values\n",
    "- Assess and correct batch effects\n",
    "- Prepare data for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session summary\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA ACQUISITION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset: GSE171140\")\n",
    "print(f\"Samples: {len(sample_mapping)}\")\n",
    "print(f\"CpG probes: {methylation_data.shape[0]:,}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - {mapping_path}\")\n",
    "print(f\"  - {info_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
